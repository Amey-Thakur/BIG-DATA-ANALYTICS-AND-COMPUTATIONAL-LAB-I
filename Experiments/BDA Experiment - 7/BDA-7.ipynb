{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hadoop.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amey-Thakur/BIG-DATA-ANALYTICS-AND-COMPUTATIONAL-LAB-I/blob/main/HADOOP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS3PVG7Cbjlw"
      },
      "source": [
        "# Install java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R8Had9fdMZB"
      },
      "source": [
        "# Install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW8h2LsQdcP6"
      },
      "source": [
        "# Unzip the spark file to the current folder\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaXbR-cRdftw"
      },
      "source": [
        "# Set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB5n8MmKdmKb"
      },
      "source": [
        "# Install findspark using pip\n",
        "!pip install -q findspark"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MiXlwPLdp6h",
        "outputId": "20f56dbe-3330-4412-90f8-429c3372238a"
      },
      "source": [
        "# Spark for Python\n",
        "!pip install pyspark"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.1.2)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1FjCOBZl3dt",
        "outputId": "982e9ad5-ac8d-4748-d9d6-ac40ea1ceb26"
      },
      "source": [
        "pip install map"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: map in /usr/local/lib/python3.7/dist-packages (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n54S6Ypa6M3"
      },
      "source": [
        "# To find out path where pyspark installed\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys2BjG4vbKSS"
      },
      "source": [
        "# Create SparkSession and sparkcontext\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "                    .master(\"local\")\\\n",
        "                    .appName('Firstprogram')\\\n",
        "                    .getOrCreate()\n",
        "sc=spark.sparkContext"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycP9gAHObOqQ"
      },
      "source": [
        "# Read the input file and Calculating words count\n",
        "text_file = sc.textFile(\"/content/sample.txt\")\n",
        "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
        "                            .map(lambda word: (word, 1)) \\\n",
        "                           .reduceByKey(lambda x, y: x + y)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6kXh8q2bYUj",
        "outputId": "13e4a18e-eaaa-446d-90ed-e752d58df002",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cat sample.txt"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This blog is exclusive for all the people who are interested in learning one of the trending in current IT industry.\r\n",
            "This course is suitable for students, teacher, IT engineer, freshers etc.,\r\n",
            "It talks about the Spark with python (pyspark) from beginner till experts."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4tymlcZbonL"
      },
      "source": [
        "# Read the input file and Calculating words count\n",
        "text_file = sc.textFile(\"/content/sample.txt\")\n",
        "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
        "                            .map(lambda word: (word, 1)) \\\n",
        "                           .reduceByKey(lambda x, y: x + y)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9_KYliabf83",
        "outputId": "1a820fce-c558-45f2-b254-6b2b732a4568",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Printing each word with its respective count\n",
        "output = counts.collect()\n",
        "for (word, count) in output:\n",
        "    print(\"%s: %i\" % (word, count))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This: 2\n",
            "blog: 1\n",
            "is: 2\n",
            "exclusive: 1\n",
            "for: 2\n",
            "all: 1\n",
            "the: 3\n",
            "people: 1\n",
            "who: 1\n",
            "are: 1\n",
            "interested: 1\n",
            "in: 2\n",
            "learning: 1\n",
            "one: 1\n",
            "of: 1\n",
            "trending: 1\n",
            "current: 1\n",
            "IT: 2\n",
            "industry.: 1\n",
            "course: 1\n",
            "suitable: 1\n",
            "students,: 1\n",
            "teacher,: 1\n",
            "engineer,: 1\n",
            "freshers: 1\n",
            "etc.,: 1\n",
            "It: 1\n",
            "talks: 1\n",
            "about: 1\n",
            "Spark: 1\n",
            "with: 1\n",
            "python: 1\n",
            "(pyspark): 1\n",
            "from: 1\n",
            "beginner: 1\n",
            "till: 1\n",
            "experts.: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLgl7X28bgrj"
      },
      "source": [
        "# Stopping Spark-Session and Spark context\n",
        "sc.stop()\n",
        "spark.stop()"
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}